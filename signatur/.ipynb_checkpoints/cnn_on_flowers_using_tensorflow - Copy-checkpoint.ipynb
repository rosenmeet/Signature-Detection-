{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']\n",
      "Types of flowers found:  5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import operator\n",
    "import functools\n",
    "import random\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from scipy.ndimage.interpolation import rotate, shift, zoom\n",
    "\n",
    "flowers_path = os.listdir('dataset/flowers')\n",
    "\n",
    "flower_types = os.listdir('dataset/flowers')\n",
    "print (flower_types)  #what kinds of flowers are in this dataset\n",
    "\n",
    "print(\"Types of flowers found: \", len(flowers_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "path = 'dataset/flowers/'\n",
    "\n",
    "\n",
    "im_size = 60\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for i in flower_types:\n",
    "    data_path = path + str(i)  # entered in daisy folder\n",
    "    filenames = [i for i in os.listdir(data_path) if i.endswith('.jpg')]\n",
    "    #print(filenames)  # will get the names of all images which ends with .jpg extension\n",
    "    for f in filenames:\n",
    "        img = cv2.imread(data_path + '/' + f)  # reading that image as array\n",
    "        #print(img)  # will get the image as an array\n",
    "        img = cv2.resize(img, (im_size, im_size))\n",
    "        images.append(img)\n",
    "        labels.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8646, 60, 60, 3)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the image array to a numpy type\n",
    "\n",
    "images = np.array(images)\n",
    "\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 4 4 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\aarohi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "c:\\users\\aarohi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8646, 5)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#images = images.astype('float32') / 255.0\n",
    "from sklearn.preprocessing import LabelEncoder , OneHotEncoder\n",
    "\n",
    "# for y\n",
    "y_labelencoder = LabelEncoder ()\n",
    "y = y_labelencoder.fit_transform (labels)\n",
    "print (y)\n",
    "#y=np.asarray(y)\n",
    "\n",
    "y=y.reshape(-1,1)\n",
    "onehotencoder = OneHotEncoder(categorical_features=[0])  #Converted  scalar output into vector output where the correct class will be 1 and other will be 0\n",
    "Y= onehotencoder.fit_transform(y)\n",
    "Y.shape #  (8646, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, Y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6916, 60, 60, 3)\n",
      "(1730, 60, 60, 3)\n",
      "(6916, 5)\n",
      "(1730, 5)\n",
      "Training shape: (6916, 60, 60, 3)\n",
      "6916 sample, 60 x 60 x 3 size rgb image.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print('Training shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'sample,',X_train.shape[1] ,'x',X_train.shape[2] ,'x',X_train.shape[3] ,'size rgb image.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train and test classification between 0-10\n",
    "#Y_test_cls = np.argmax(y_test, axis=1)\n",
    "#Y_train_cls = np.argmax(y_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (images) shape: (6916, 10800)\n",
      "Training set (labels) shape: (6916, 5)\n",
      "Test set (images) shape: (1730, 10800)\n",
      "Test set (labels) shape: (1730, 5)\n"
     ]
    }
   ],
   "source": [
    "img_size=60\n",
    "\n",
    "\n",
    "# Number of colour channels for the images.\n",
    "num_channels = 3\n",
    "\n",
    "img_size_flat = img_size * img_size * num_channels\n",
    "\n",
    "# Tuple with height and width of images used to reshape arrays.\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "\n",
    "\n",
    "# Number of classes, one class for each of 5 classes.\n",
    "num_classes = 5\n",
    "\n",
    "new_train_X = X_train.reshape(X_train.shape[0],img_size_flat)\n",
    "new_test_X = X_test.reshape(X_test.shape[0],img_size_flat)\n",
    "# Shapes of training set\n",
    "print(\"Training set (images) shape: {shape}\".format(shape=new_train_X.shape))\n",
    "print(\"Training set (labels) shape: {shape}\".format(shape=y_train .shape))\n",
    "\n",
    "# Shapes of test set\n",
    "print(\"Test set (images) shape: {shape}\".format(shape=new_test_X.shape))\n",
    "print(\"Test set (labels) shape: {shape}\".format(shape=y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture hyper-parameter\n",
    "learning_rate = 0.001\n",
    "training_iters = 40000\n",
    "batch_size = 16\n",
    "display_step = 20\n",
    "\n",
    "n_input = img_size_flat # 60x60 image\n",
    "dropout = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of placeholder (?, 10800) (?, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "print('Shape of placeholder',x.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(x, weights, biases, dropout):\n",
    "    # reshape input to 60x60x3 size\n",
    "    x = tf.reshape(x, shape=[-1, 60, 60, 3])\n",
    "    \n",
    "\n",
    "    # Convolution layer 1\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max pooling\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution layer 2\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max pooling\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1) # layer\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 3, 32]),name='wc1'),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64]),name='wc2'),\n",
    "    'wd1': tf.Variable(tf.random_normal([64 * 64 * 4, 1024]),name='wd1'),\n",
    "    'out': tf.Variable(tf.random_normal([1024, num_classes]),name='wout')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32]),name='bc1'),\n",
    "    'bc2': tf.Variable(tf.random_normal([64]),name='bc2'),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024]),name='bd1'),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]),name='bout')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add_5:0\", shape=(?, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = conv_net(x, weights, biases, keep_prob)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_true_cls =  tf.argmax(y, 1)\n",
    "y_pred_cls = tf.argmax(model, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a vector of booleans whether the predicted \n",
    "#class equals the true class of each image.\n",
    "correct_model = tf.equal(y_pred_cls,y_true_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This calculates the classification accuracy by first type-casting \n",
    "#the vector of booleans to floats, so that False becomes 0 and True becomes 1,\n",
    "#and then calculating the average of these numbers.\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_model, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatch(X, Y, batchSize=16):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input train/test \n",
    "    Y --input label train/test\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- tuple of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \n",
    "    \"\"\"\n",
    "    arrayLength = X.shape[0]\n",
    "    count = 0 \n",
    "    \n",
    "    while count < arrayLength/batchSize:\n",
    "        random.seed(datetime.datetime.now())\n",
    "        randstart = random.randint(0, arrayLength-batchSize-1)\n",
    "#         print(randstart)\n",
    "        count += 1\n",
    "        yield (X[randstart:randstart+batchSize], Y[randstart:randstart+batchSize]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-4779a7fd2b13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_train_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'*'\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aarohi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aarohi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1140\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1142\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32mc:\\users\\aarohi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "loss_t = []\n",
    "steps_t = []\n",
    "acc_t = []\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1   \n",
    "#     Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        a = getBatch(new_train_X,y_train, batch_size)\n",
    "        batch_x, batch_y = next(a)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "        if step % display_step == 0:\n",
    "            print('*'*15)\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
    "                                                              y: batch_y,\n",
    "                                                              keep_prob: 1.})\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Loss= \" + \\\n",
    "                  \"{:.3f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "            loss_t.append(loss)\n",
    "            steps_t.append(step*batch_size)\n",
    "            acc_t.append(acc)\n",
    "        step += 1\n",
    "    \n",
    "   #\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: new_test_X,\n",
    "                                      y: y_test,\n",
    "                                      keep_prob: 1.}))\n",
    "    \n",
    "    cls_pred = sess.run(y_pred_cls, feed_dict={x: new_test_X,\n",
    "                                      y: Y_test,\n",
    "                                      keep_prob: 1.})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(steps_t, loss_t, 'r--')\n",
    "plt.xlabel(\"Number of Iterarion\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
